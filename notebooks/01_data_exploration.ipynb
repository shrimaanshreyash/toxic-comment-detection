{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Collection & Exploration\n",
    "\n",
    "**Goal**: Load and understand the Toxic Comment Classification dataset\n",
    "\n",
    "## Dataset: Kaggle Toxic Comment Classification Challenge\n",
    "\n",
    "### Why This Dataset?\n",
    "\n",
    "1. **Real-world data**: Wikipedia talk page comments\n",
    "2. **Multi-label**: 6 toxicity categories\n",
    "3. **Scale**: 159,571 labeled comments\n",
    "4. **Well-documented**: Used in research and industry\n",
    "5. **Publicly available**: Can be shared in portfolio\n",
    "\n",
    "### Toxicity Categories\n",
    "- `toxic`: Rude, disrespectful comments\n",
    "- `severe_toxic`: Very hateful, aggressive, disrespectful comments\n",
    "- `obscene`: Vulgar or profane language\n",
    "- `threat`: Threatening language toward an individual or group\n",
    "- `insult`: Insulting or negative comments\n",
    "- `identity_hate`: Hate speech targeting identity (race, religion, gender, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic cell - Run this first to verify your environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Notebook directory: {os.path.dirname(os.path.abspath('.'))}\")\n",
    "\n",
    "# Check if data file exists\n",
    "data_path = '../data/raw/train.csv'\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"\\n✓ Data file found: {data_path}\")\n",
    "    file_size = os.path.getsize(data_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"  File size: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n✗ Data file NOT found: {data_path}\")\n",
    "    print(\"  Please ensure the dataset is in the correct location\")\n",
    "\n",
    "# Check if results directory exists\n",
    "results_dir = '../results/figures'\n",
    "if not os.path.exists(results_dir):\n",
    "    print(f\"\\n⚠ Creating results directory: {results_dir}\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "else:\n",
    "    print(f\"\\n✓ Results directory exists: {results_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"If you see this output, your notebook is running correctly!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import display for Jupyter notebooks\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    # Fallback if not in Jupyter environment\n",
    "    display = print\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a: Load Dataset\n",
    "\n",
    "**Note**: Download the dataset from Kaggle:\n",
    "1. Go to: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "2. Download `train.csv` and `test.csv`\n",
    "3. Place in `../data/raw/` directory\n",
    "\n",
    "Alternatively, use the Kaggle API:\n",
    "```bash\n",
    "pip install kaggle\n",
    "kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# If you don't have the dataset yet, we'll create a sample for demonstration\n",
    "\n",
    "try:\n",
    "    # Try to load the actual dataset\n",
    "    df = pd.read_csv('../data/raw/train.csv')\n",
    "    print(\"✓ Loaded actual Kaggle dataset\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Creating sample dataset for demonstration...\")\n",
    "    # Create a sample dataset for demonstration\n",
    "    sample_data = {\n",
    "        'id': range(1000),\n",
    "        'comment_text': [\n",
    "            \"This is a great article, thank you!\",\n",
    "            \"You're an idiot and nobody likes you\",\n",
    "            \"I disagree with this perspective\",\n",
    "            \"This is complete garbage written by morons\",\n",
    "            \"Interesting point, could you elaborate?\",\n",
    "        ] * 200,  # Repeat to get 1000 samples\n",
    "        'toxic': [0, 1, 0, 1, 0] * 200,\n",
    "        'severe_toxic': [0, 0, 0, 1, 0] * 200,\n",
    "        'obscene': [0, 1, 0, 1, 0] * 200,\n",
    "        'threat': [0, 0, 0, 0, 0] * 200,\n",
    "        'insult': [0, 1, 0, 1, 0] * 200,\n",
    "        'identity_hate': [0, 0, 0, 0, 0] * 200,\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(\"✓ Created sample dataset for demonstration\")\n",
    "    print(\"⚠ Replace with actual Kaggle dataset for complete analysis\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of comments: {len(df):,}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b: Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"-\" * 40)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"\\n✓ No missing values found!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Total missing values: {missing_df['Missing Count'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text characteristics\n",
    "print(\"TEXT CHARACTERISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate text lengths\n",
    "df['text_length'] = df['comment_text'].astype(str).apply(len)\n",
    "df['word_count'] = df['comment_text'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "print(f\"\\nComment Length Statistics:\")\n",
    "print(f\"  Average characters: {df['text_length'].mean():.0f}\")\n",
    "print(f\"  Median characters: {df['text_length'].median():.0f}\")\n",
    "print(f\"  Min characters: {df['text_length'].min()}\")\n",
    "print(f\"  Max characters: {df['text_length'].max()}\")\n",
    "\n",
    "print(f\"\\nWord Count Statistics:\")\n",
    "print(f\"  Average words: {df['word_count'].mean():.1f}\")\n",
    "print(f\"  Median words: {df['word_count'].median():.0f}\")\n",
    "print(f\"  Min words: {df['word_count'].min()}\")\n",
    "print(f\"  Max words: {df['word_count'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "print(\"\\nCLASS DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define toxicity columns\n",
    "toxicity_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Calculate distribution for each category\n",
    "print(\"\\nToxicity by Category:\")\n",
    "print(\"-\" * 40)\n",
    "for col in toxicity_cols:\n",
    "    count = df[col].sum()\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"{col:20s}: {count:6d} ({pct:5.2f}%)\")\n",
    "\n",
    "# Check if any comment is toxic in any category\n",
    "df['any_toxic'] = (df[toxicity_cols].sum(axis=1) > 0).astype(int)\n",
    "toxic_count = df['any_toxic'].sum()\n",
    "clean_count = len(df) - toxic_count\n",
    "\n",
    "print(\"\\nOverall Distribution:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Clean comments:        {clean_count:6d} ({(clean_count/len(df)*100):5.2f}%)\")\n",
    "print(f\"Toxic comments:        {toxic_count:6d} ({(toxic_count/len(df)*100):5.2f}%)\")\n",
    "print(f\"\\nClass imbalance ratio: {clean_count/toxic_count:.2f}:1 (clean:toxic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Overall toxic vs clean\n",
    "ax1 = axes[0, 0]\n",
    "counts = df['any_toxic'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "ax1.pie(counts, labels=['Clean', 'Toxic'], autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90)\n",
    "ax1.set_title('Overall Distribution: Clean vs Toxic', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Bar chart of each category\n",
    "ax2 = axes[0, 1]\n",
    "category_counts = df[toxicity_cols].sum().sort_values(ascending=True)\n",
    "ax2.barh(category_counts.index, category_counts.values, color='coral')\n",
    "ax2.set_xlabel('Number of Comments', fontsize=11)\n",
    "ax2.set_title('Toxicity by Category', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Percentage by category\n",
    "ax3 = axes[1, 0]\n",
    "category_pct = (df[toxicity_cols].sum() / len(df) * 100).sort_values(ascending=True)\n",
    "ax3.barh(category_pct.index, category_pct.values, color='steelblue')\n",
    "ax3.set_xlabel('Percentage (%)', fontsize=11)\n",
    "ax3.set_title('Toxicity Rate by Category', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Multi-label distribution\n",
    "ax4 = axes[1, 1]\n",
    "label_counts = df[toxicity_cols].sum(axis=1).value_counts().sort_index()\n",
    "ax4.bar(label_counts.index, label_counts.values, color='mediumpurple', alpha=0.7)\n",
    "ax4.set_xlabel('Number of Toxic Labels', fontsize=11)\n",
    "ax4.set_ylabel('Number of Comments', fontsize=11)\n",
    "ax4.set_title('Multi-Label Distribution', fontsize=14, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved to: results/figures/class_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1c: Understand Abuse Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multi-label patterns\n",
    "print(\"MULTI-LABEL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# How many labels does each toxic comment have?\n",
    "toxic_df = df[df['any_toxic'] == 1].copy()\n",
    "toxic_df['num_labels'] = toxic_df[toxicity_cols].sum(axis=1)\n",
    "\n",
    "print(\"\\nLabel Count Distribution (for toxic comments):\")\n",
    "print(\"-\" * 40)\n",
    "label_dist = toxic_df['num_labels'].value_counts().sort_index()\n",
    "for num_labels, count in label_dist.items():\n",
    "    pct = (count / len(toxic_df)) * 100\n",
    "    print(f\"  {num_labels} label(s): {count:5d} ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nAverage labels per toxic comment: {toxic_df['num_labels'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label correlations\n",
    "print(\"\\nLABEL CORRELATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[toxicity_cols].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Between Toxicity Categories', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/toxicity_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Correlations (top 5):\")\n",
    "print(\"-\" * 40)\n",
    "# Get upper triangle of correlation matrix\n",
    "corr_pairs = []\n",
    "for i in range(len(toxicity_cols)):\n",
    "    for j in range(i+1, len(toxicity_cols)):\n",
    "        corr_pairs.append((toxicity_cols[i], toxicity_cols[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Sort by correlation value\n",
    "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "for label1, label2, corr_val in corr_pairs_sorted[:5]:\n",
    "    print(f\"  {label1:15s} <-> {label2:15s}: {corr_val:.3f}\")\n",
    "\n",
    "print(\"\\n✓ Correlation heatmap saved to: results/figures/toxicity_correlations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample comments from each category\n",
    "print(\"\\nSAMPLE COMMENTS BY CATEGORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in toxicity_cols:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    # Get comments that are ONLY this category (if possible)\n",
    "    single_label = df[(df[col] == 1) & (df[toxicity_cols].sum(axis=1) == 1)]\n",
    "    \n",
    "    if len(single_label) > 0:\n",
    "        sample = single_label.sample(min(2, len(single_label)))\n",
    "    else:\n",
    "        # If no single-label examples, get any examples\n",
    "        sample = df[df[col] == 1].sample(min(2, df[col].sum()))\n",
    "    \n",
    "    for idx, row in sample.iterrows():\n",
    "        text = row['comment_text'][:150]  # Truncate for readability\n",
    "        if len(row['comment_text']) > 150:\n",
    "            text += \"...\"\n",
    "        print(f\"  • {text}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "print(\"\\nDATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated(subset=['comment_text']).sum()\n",
    "print(f\"\\nDuplicate comments: {duplicates} ({(duplicates/len(df)*100):.2f}%)\")\n",
    "\n",
    "# Check for empty comments\n",
    "empty_comments = (df['comment_text'].astype(str).str.strip() == '').sum()\n",
    "print(f\"Empty comments: {empty_comments}\")\n",
    "\n",
    "# Check for very short comments (< 10 characters)\n",
    "very_short = (df['text_length'] < 10).sum()\n",
    "print(f\"Very short comments (<10 chars): {very_short} ({(very_short/len(df)*100):.2f}%)\")\n",
    "\n",
    "# Check for very long comments (> 1000 characters)\n",
    "very_long = (df['text_length'] > 1000).sum()\n",
    "print(f\"Very long comments (>1000 chars): {very_long} ({(very_long/len(df)*100):.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ Dataset is ready for preprocessing\")\n",
    "print(f\"✓ Total samples: {len(df):,}\")\n",
    "print(f\"✓ Toxic samples: {toxic_count:,} ({(toxic_count/len(df)*100):.1f}%)\")\n",
    "print(f\"✓ Clean samples: {clean_count:,} ({(clean_count/len(df)*100):.1f}%)\")\n",
    "print(f\"✓ Categories: {len(toxicity_cols)}\")\n",
    "print(f\"✓ No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataframe for next notebook\n",
    "df.to_csv('../data/processed/data_explored.csv', index=False)\n",
    "print(\"\\n✓ Processed data saved to: data/processed/data_explored.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings from Data Exploration\n",
    "\n",
    "### 1. Dataset Characteristics\n",
    "- **Size**: 159,571 comments (or your dataset size)\n",
    "- **Source**: Wikipedia talk page discussions\n",
    "- **Quality**: No missing values, minimal data quality issues\n",
    "\n",
    "### 2. Class Distribution\n",
    "- **Imbalanced dataset**: ~90% clean, ~10% toxic (typical for real-world content)\n",
    "- **Most common**: 'toxic' and 'obscene' categories\n",
    "- **Least common**: 'threat' and 'identity_hate' categories\n",
    "\n",
    "### 3. Multi-Label Nature\n",
    "- Comments can have multiple toxicity types\n",
    "- Average of 1-2 labels per toxic comment\n",
    "- Strong correlations between certain categories (e.g., obscene + insult)\n",
    "\n",
    "### 4. Text Characteristics\n",
    "- Toxic comments tend to be longer than clean comments\n",
    "- Wide range of comment lengths (from a few words to paragraphs)\n",
    "\n",
    "### Next Steps\n",
    "1. Preprocess text (cleaning, tokenization)\n",
    "2. Extract features (TF-IDF, word embeddings)\n",
    "3. Conduct exploratory data analysis\n",
    "4. Build detection framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
